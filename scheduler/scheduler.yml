# Copyright 2025 Dell Inc. or its subsidiaries. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---

- name: Set_fact for fetch omnia config credentials
  hosts: localhost
  connection: local
  tasks:
    - name: Set dynamic run tags including 'slurm'
      when: not (hostvars['127.0.0.1']['config_file_status'] | default(false) | bool)
      ansible.builtin.set_fact:
        omnia_run_tags: "{{ (ansible_run_tags | default([]) + ['slurm']) | unique }}"
        cacheable: true

- name: Invoke get_config_credentials.yml
  ansible.builtin.import_playbook: ../utils/credential_utility/get_config_credentials.yml
  when: not ( hostvars['127.0.0.1']['config_file_status'] | default(false) | bool )

- name: Include input project directory
  ansible.builtin.import_playbook: ../utils/include_input_dir.yml
  when: not ( hostvars['127.0.0.1']['project_dir_status'] | default(false) | bool )

- name: Validate inventory
  ansible.builtin.import_playbook: ../utils/inventory_validation.yml
  when: not ( hostvars['127.0.0.1']['inventory_validation_executed'] | default(false) | bool )

- name: Update Inventory with ansible_host information
  ansible.builtin.import_playbook: ../utils/servicetag_host_mapping.yml
  when: not ( hostvars['127.0.0.1']['update_inventory_executed'] | default(false) | bool )

- name: Gather facts from all the nodes
  hosts: slurm_control_node, slurm_node, slurm_dbd, login, kube_control_plane, kube_node, etcd
  gather_facts: true

- name: Validate scheduler input parameters
  hosts: localhost
  connection: local
  gather_facts: false
  tasks:
    - name: Validate scheduler input parameters
      ansible.builtin.include_role:
        name: cluster_validation
        tasks_from: validation_status_check.yml

- name: Create kubespray container group
  ansible.builtin.import_playbook: ../utils/create_container_group.yml
  vars:
    omnia_kubespray_group: true
    omnia_kubespray_validation: true
  when: hostvars['127.0.0.1']['k8s_support']

- name: Prepare kube control plane for kubernetes installations
  hosts: kube_control_plane
  gather_facts: true
  roles:
    - k8s_manager

- name: Prepare kube control plane, kube nodes and etcd for kubernetes installations
  hosts: kube_control_plane, kube_node, etcd
  gather_facts: false
  any_errors_fatal: true
  roles:
    - k8s_prepare_nodes

- name: Start Kubernetes Installations
  hosts: omnia_kubespray
  vars:
    kubespray_nfs_share: "/opt/omnia/kubespray"
    kubespray_logs_path: "/opt/omnia/log/kubespray/k8s_deployment.log"
    dir_mode: '0755'
  tasks:
    - name: Install Kubernetes using kubespray
      when:
        - hostvars['127.0.0.1']['k8s_support']
        - not hostvars[groups['kube_control_plane'][0]]['k8s_installation_status']
      block:
        - name: Copy k8s_start_setup to kubespray nfs share
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/playbooks/k8s_start_setup.yml"
            dest: "{{ kubespray_nfs_share }}/k8s_start_setup.yml"
            mode: "{{ dir_mode }}"

        - name: Execute ansible-playbook for Kubernetes setup asynchronously
          ansible.builtin.shell: |
            set -o pipefail
            /venv/bin/ansible-playbook {{ kubespray_nfs_share }}/k8s_start_setup.yml \
            -i {{ kubespray_nfs_share }}/inv_k8s \
            --extra-vars "@{{ kubespray_nfs_share }}/k8s_all_vars.yml" -vvv | \
            /usr/bin/tee {{ kubespray_logs_path }}
          async: 3600  # Set async timeout (e.g., 1 hour)
          poll: 0  # Non-blocking (continue the playbook without waiting for completion)
          register: result  # Register the result to capture job ID
          changed_when: false

        - name: Wait for the Kubernetes installation to finish. Logs can be checked at {{ kubespray_logs_path }}
          ansible.builtin.async_status:
            jid: "{{ result.ansible_job_id }}"  # Job ID from the previous task
          register: job_result
          until: job_result.finished
          retries: 60  # Retry the task 60 times (10 min total)
          delay: 10  # Wait 10 seconds between retries

- name: Add nodes to kubernetes cluster
  hosts: omnia_kubespray
  vars:
    kubespray_nfs_share: "/opt/omnia/kubespray"
    kubespray_logs_path: "/opt/omnia/log/kubespray/k8s_add_node.log"
    dir_mode: '0755'
  tasks:
    - name: Add nodes to kubernetes cluster using kubespray
      when:
        - hostvars['127.0.0.1']['k8s_support']
        - hostvars[groups['kube_control_plane'][0]]['k8s_installation_status']
        - hostvars[groups['kube_control_plane'][0]]['k8s_new_nodes'] != ''
      block:
        - name: Copy k8s_add_node to kubespray nfs share
          ansible.builtin.copy:
            src: "{{ playbook_dir }}/playbooks/k8s_add_node.yml"
            dest: "{{ kubespray_nfs_share }}/k8s_add_node.yml"
            mode: "{{ dir_mode }}"

        - name: Execute ansible-playbook for Kubernetes setup asynchronously
          ansible.builtin.shell: |
            set -o pipefail
            /venv/bin/ansible-playbook {{ kubespray_nfs_share }}/k8s_add_node.yml \
              -i {{ kubespray_nfs_share }}/inv_k8s \
              --extra-vars "@{{ kubespray_nfs_share }}/k8s_all_vars.yml" -vvv | \
              /usr/bin/tee {{ kubespray_logs_path }}
          args:
            executable: /bin/bash  # Ensures bash is used for `set -o pipefail`
          async: 3600  # Run asynchronously for up to 1 hour
          poll: 0  # Non-blocking execution
          register: result  # Capture the job ID
          changed_when: false

        - name: Wait for the Kubernetes installation to finish. Logs can be checked at {{ kubespray_logs_path }}
          ansible.builtin.async_status:
            jid: "{{ result.ansible_job_id }}"  # Job ID from the previous task
          register: job_result
          until: job_result.finished
          retries: 60  # Retry the task 60 times (10 min total)
          delay: 10  # Wait 10 seconds between retries

- name: Update containerd config on nodes
  hosts: kube_control_plane, kube_node
  gather_facts: false
  roles:
    - update_containerd_config

- name: Start K8s worker servers on kube control nodes
  hosts: kube_control_plane
  gather_facts: false
  roles:
    - k8s_start_services
    - multus_and_whereabouts

- name: Install Slurm
  hosts: slurm_control_node, slurm_node, login
  any_errors_fatal: true
  roles:
    - slurm

- name: Install Slurm_pam
  hosts: slurm_node
  any_errors_fatal: true
  roles:
    - slurm_pam

- name: Compile and install the ucx and openmpi on the nfs share of compute nodes
  hosts: slurm_control_node, kube_control_plane
  gather_facts: true
  roles:
    - install_benchmarks_tools
